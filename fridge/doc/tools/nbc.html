<h1><join> NBC: a not-so-naive Bayes Classifier</join></h1>
<h2>Contents</h2>
<div id="htmltoc">
<h2><font color=black>&bull;</font></a> <a href=#1>Synopsis</a></h2>
<h2><font color=black>&bull;</font></a> <a href=#2>Download</a></h2>
<h2><font color=black>&bull;</font></a> <a href=#3>Description</a></h2>
<h3><font color=black>&bull;</font></a> <a href=#4>Summary of Results</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#5>Details: Accuracy</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#6>Details: Runtimes</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#7>Details: Memory</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#8>Discussion</a></h3>
<h2><font color=black>&bull;</font></a> <a href=#9>Options</a></h2>
<h2><font color=black>&bull;</font></a> <a href=#10>Installation</a></h2>
<h2><font color=black>&bull;</font></a> <a href=#11>Awk code</a></h2>
<h2><font color=black>&bull;</font></a> <a href=#12>Bash Code</a></h2>
<h3><font color=black>&bull;</font></a> <a href=#13>Copyright</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#14>Usage</a></h3>
<h3><font color=black>&bull;</font></a> <a href=#15>Demo code</a></h3>
<h2><font color=black>&bull;</font></a> <a href=#16>Main</a></h2>
<h3><font color=black>&bull;</font></a> <a href=#17>Command-line Processing</a></h3>
<h2><font color=black>&bull;</font></a> <a href=#18>Author</a></h2>
</div><!--- htmltoc --->
<div id="htmlbody">
<a name=1></a><H2> Synopsis</H2>
<PRE>
 ./nbc -[hlx] train test
</PRE>
<a name=2></a><H2> Download</H2>
<P>
Download from
<a href="http://lawker.googlecode.com/svn/fridge/lib/bash/nbc/contents.zip">Lawker</a>
<a name=3></a><H2> Description</H2>
<P>
Is less more? Can a few lines of gawk developed in a day or
two stand in for a sophisticated
state-of-the-art JAVA package?

In the general
case there may be software engineering advantages to working with 
rich languages like JAVA. However, in the specific case of
a Naive Bayes classifier for discrete data, it is interesting
to test if less is indeed more.
<P>
A Naive Bayes classifier collects frequency counts of old events, grouped into "classes".
Then, if a new event arrives without a classification, it checks through the old list of classes
looking for the one with the highest frequency counts for this new event. 
<P>
The method is called "naive"
This assumption allows us to collect frequency counts just on each attribute value (and not
pairs, or triples, or quads of values).
<P>
In practice,
this "naive" strategy works remarkably well- often performs as well as other schemes that try to model
interactions between frequencies.
<P>
Hence, we call this system a not-so-naive Bayes Classifier.
<a name=4></a><H3> Summary of Results</H3>
<P>
In summary, the performance on this simple gawk-based Naive Bayes classifier is quite remarkable.
<UL>
<LI>
Not surprisingly, nbc.awk's accuracy are very 
   similar to a standard bayes classifier (from the WEKA
system).  That is, this implementation is <em>adequate</em>.
<LI>
Quite surprisingly, the
awk version does not run much slower than java. In fact,
for under 1000 instances, this implementation is
comparatively  <em>fast or faster</em> than a much more mature
JAVA-based tool.
</UL>
<a name=5></a><H3> Details: Accuracy</H3>
<P>
The following table
compares classification accuracies between nbc.awk
and WEKA's
weka.classifiers.NaiveBayes. 
<P>
All the data sets
were discrete so no kernel estimation was used.
<P>
Results come from a   10-way
cross-val (but no initial randomization of data set order). 
<P>
The table is sorted by
increase mean difference.  
Nbc.awk does better than WEKA Bayes on
the datasets shown at the bottom of the table. 
<PRE>
                mean                significant 
                difference   std.   difference? 
      data      in accuracy  dev.   (alpha=0.01)
 -----------------------------------------------
        soybean | -3.02 |   2.02 |  y
           iris | -2.14 |   4.51 |  n
            zoo | -0.38 |   6.54 |  n
  primary-tumor | -0.30 |   3.28 |  n
      audiology | -0.27 |   2.67 |  n
       mushroom | -0.25 |   0.23 |  n
         splice |  0.00 |   0.00 |  n
       kr-vs-kp |  0.00 |   0.00 |  n
  breast-cancer |  0.00 |   0.00 |  n
 contact-lenses |  0.00 |   0.00 |  n
           vote |  0.27 |   1.47 |  n
          lymph |  0.73 |   5.01 |  n
       breast-w |  1.63 |   1.32 |  y
       credit-a |  7.40 |   5.60 |  y
         letter |  9.44 |   1.40 |  y
</PRE>
<P>
On the whole, nbc.awk works as well as WEKA Bayes.
<a name=6></a><H3> Details: Runtimes</H3>
<P>
The following table compares the runtimes of
nbc.awk  (awk) vs WEKA BAYES (java) measured in
seconds. 
<P>
Each lines show total times for ten training+test runs (one for each
item in the cross val). E.g. letter actually ran in time 4.92 seconds (on average)
and this was called 10 times.
<P>
Note: the time for dividing files for the  x-val is not shown.
<P>
The table is sorted on the ratio of awk vs java runtimes.
Ratios less than one mean awk ran faster than java.
Sampler.awk does better than Weka Bayes on the datasets shown
at the bottom of the table (below the middle line).
<PRE>
                 runtimes (secs) | 
                -----------------|---------------------
           data  awk   java ratio| insts  attrs classes
 --------------------------------|---------------------
         letter  49.2  17.6  2.8  20,000   17      27
       mushroom  10.1   5.9  1.7   8,124   23       3
       kr-vs-kp   8.1   5.1  1.6   3,916   37       3
         splice  11.3   7.8  1.4   3,190   62       4
        soybean   4.2   3.4  1.2     683   36      20
 ------------------------------------------------------
      audiology   2.9   3.4  0.9     226   70      25
  primary-tumor   1.3   2.8  0.5     339   18      23
           vote   1.0   2.4  0.4     435   17       3
 contact-lenses   0.6   2.0  0.3      24    5       4
  breast-cancer   0.7   2.4  0.3     286   10       3
       credit-a   1.1   3.3  0.3     690   16       3
       breast-w   1.0   3.5  0.3     699   10       3
          lymph   0.6   2.4  0.2     148   19       5
           iris   0.5   2.5  0.2     150    5       4
            zoo   0.6   2.4  0.2     101   18       8
 ------------------------------------------------------
          total  93.1  66.8  1.4
</PRE>
<P>
All up,
 the awk-based learner was 40% slower than the JAVA.
For larger data sets, JAVA was always faster. However, for smaller
 datasets (under 1000 instances) the awk version was nearly as fast or faster.
<a name=7></a><H3> Details: Memory</H3>
<P>
We have run this small Awk script on 100s of megabytes of data, without crashes or core dumps.
The code is very memory effecient- unlike the WEKA which loads all the data into RAM.
<a name=8></a><H3> Discussion</H3>
<P>
It is hardly surprising that a state-of-the-art tool kit built and optimized
by JAVA gurus can out-perform awk code on large examples. However, what is surprising is
that an 32 line AWK script built and debugged in a weekend often works nearly
as well, or better. 
<P>
Perhaps "nbc" is not-so-naive after all.
<a name=9></a><H2> Options</H2>
<DL>
<DT> -x</DT>
<DD> run an example</DD>
<DT> -h</DT>
<DD> print help text</DD>
<DT> -l</DT>
<DD> print legal notice</DD>
</DL>
<a name=10></a><H2> Installation</H2>
<P>
To check the download, unzip the contents.zip then
<PRE>
 chmod +x nbc
 ./nbc nbceg.train nbceg.test  | 
 gawk -F, '{print $0  "\t " ($1 !=$2 ? " &lt;== bad" : "")}' 
</PRE>
<P> This should print:</P>
<PRE>
 malign_lymph,malign_lymph        
 metastases,metastases    
 malign_lymph,malign_lymph        
 metastases,metastases    
 malign_lymph,metastases   &lt;== bad
 malign_lymph,malign_lymph        
 malign_lymph,malign_lymph        
 metastases,metastases    
 metastases,metastases    
 metastases,metastases    
 malign_lymph,malign_lymph        
 metastases,metastases    
 malign_lymph,malign_lymph        
</PRE>
<a name=11></a><H2> Awk code</H2>
<P> Here is the nbc.awk code called by the Bash script (shown below).</P>
<PRE>
 BEGIN {
  #Internal globals:
     Total=0    # count of all instances
   # Classes    # table of class names/frequencies
   # Freg       # table of counters for values in attributes in classes
   # Seen       # table of counters for values in attributes
   # Attributes # table of number of values per attribute
   }

 Pass==1 {train()}
 Pass==2 {print $NF "," classify()}

 function train(    i,c) { 
   Total++;
   c=$NF;
   Classes[c]++;
   for(i=1;i&lt;=NF;i++) {
     if ($i=="?") continue;
     Freq[c,i,$i]++
     if (++Seen[i,$i]==1) Attributes[i]++}
 }

 function classify(         i,temp,what,like,c) {  
   like = -100000; # smaller than any log
   for(c in Classes) {  
     temp=log(Classes[c]/Total); #uses logs to stop numeric errors
     for(i=1;i&lt;NF;i++) {  
       if ( $i=="?" ) continue;
       temp += log((Freq[c,i,$i]+1)/(Classes[c]+Attributes[i]));
     };
     if ( temp >= like ) {like = temp; what=c}
   };
   return what;
 }
</PRE>
<a name=12></a><H2> Bash Code</H2>
<a name=13></a><H3> Copyright</H3>
<PRE>
copyleft() { cat&lt;&lt;EOF 
nbc: a naive bayes classifier
Copyright (C) 2004 Tim Menzies

This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation, version 2.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
EOF
}
</PRE>
<a name=14></a><H3> Usage</H3>
<PRE>
 usage() { cat&lt;&lt;-EOF
	Usage: nbc  [FLAGs] TRAIN TEST
	Naive bayes classifier.

	TRAIN and TEST are comma-separated data files with the same
	number of columns. The last column of each is the class
	symbol. This classifier learns from TRAIN and then tries
	to classify the examples in TEST.

	Flags: 
	  -h        print this help text
	  -l        copyright notice
	  -x	    run an example
	EOF
	exit
 } 
</PRE>
<a name=15></a><H3> Demo code</H3>
<PRE>
 nbcDemo() { 
	main nbceg.train nbceg.test
 } 
</PRE>
<a name=16></a><H2> Main</H2>
<PRE>
 main() {
	gawk -F, -f nbc.awk Pass=1 $1 Pass=2 $2
 }
</PRE>
<a name=17></a><H3> Command-line Processing</H3>
<PRE>
 demo=""
 while getopts "hlx" flag
 do case "$flag" in
        l)  copyleft; exit;;
        h)  usage; exit ;;
        x)  demo="nbcDemo";;
    esac
 done
 shift $(($OPTIND - 1))
 if [ -n "$demo" ]
 then $demo
      exit
 else  main $1 $2
 fi
</PRE>
<a name=18></a><H2> Author</H2>
<P> Tim Menzies</P>
</div><!--- htmlbody --->
